## Descrição: Crawler com busca de x páginas, com palavras chaves, respeitando as regras de robots.txt.

Apresentação:

O Crawler é um rastreador de páginas que faz a requisição de múltiplas páginas com o objetivo de reunir conteúdo relevante para que seja extraído dados valiosos para o mercado, como  por exemplo buscar o preço de um determinado produto, em sites de lojas concorrentes para comparar o preço com a sua loja, ou mesmo verificar o histórico de preços. O meu crawler foi desenvolvido para reunir páginas de lojas e extrair um número que pode ser facilmente alterado através de uma variável. Ele respeita as condições impostas pelos web-sites como o robots.txt. Utiliza o conceito de conjuntos, cria pastas onde irá salvar os links das páginas em questão, podendo inclusive parar o processo e posteriormente continuar.
